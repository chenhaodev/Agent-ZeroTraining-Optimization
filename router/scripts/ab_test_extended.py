#!/usr/bin/env python3
"""
Extended A/B Test: Baseline vs Router with 20+ questions
Tests across all categories with varying difficulty levels
"""
import sys
import json
from pathlib import Path
from datetime import datetime
from loguru import logger
import time

# Add repo root to path
repo_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(repo_root))

from autoeval.services.api_client import get_api_client
from router.core.decision_engine import get_decision_engine
from optimizer.core.pattern_storage import PatternStorage


# Extended test set with 20+ questions across all categories
EXTENDED_TEST_QUESTIONS = [
    # ========== DISEASES (8 questions) ==========
    {
        "question": "è†å…³èŠ‚åŠæœˆæ¿æŸä¼¤æ˜¯ä»€ä¹ˆç—…ï¼Œä¼šæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "medium",
        "expected_weaknesses": ["conservative_treatment_details", "concurrent_injuries"]
    },
    {
        "question": "ç³–å°¿ç—…æ‚£è€…å¹³æ—¶è¦æ³¨æ„ä»€ä¹ˆé¥®é£Ÿç¦å¿Œï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "easy",
        "expected_weaknesses": ["lifestyle_details", "specific_recommendations"]
    },
    {
        "question": "é«˜è¡€å‹ç—…äººå¯ä»¥è¿åŠ¨å—ï¼Ÿä»€ä¹ˆè¿åŠ¨æ¯”è¾ƒåˆé€‚ï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "medium",
        "expected_weaknesses": ["exercise_contraindications", "intensity_guidelines"]
    },
    {
        "question": "ç±»é£æ¹¿å…³èŠ‚ç‚èƒ½æ²»å¥½å—ï¼Ÿéœ€è¦é•¿æœŸåƒè¯å—ï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "hard",
        "expected_weaknesses": ["prognosis_details", "medication_duration"]
    },
    {
        "question": "ç—›é£å‘ä½œæ—¶åº”è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "medium",
        "expected_weaknesses": ["acute_management", "medication_specifics"]
    },
    {
        "question": "è„‚è‚ªè‚éœ€è¦æ²»ç–—å—ï¼Ÿä¼šä¸ä¼šå‘å±•æˆè‚ç™Œï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "medium",
        "expected_weaknesses": ["progression_risk", "treatment_necessity"]
    },
    {
        "question": "ç”²çŠ¶è…ºç»“èŠ‚æ˜¯ç™Œç—‡å—ï¼Ÿéœ€è¦æ‰‹æœ¯å—ï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "hard",
        "expected_weaknesses": ["malignancy_criteria", "treatment_indications"]
    },
    {
        "question": "è¿‡æ•æ€§é¼»ç‚å¯ä»¥æ ¹æ²»å—ï¼Ÿ",
        "entity_type": "disease",
        "difficulty": "easy",
        "expected_weaknesses": ["cure_possibility", "long_term_management"]
    },

    # ========== EXAMINATIONS (6 questions) ==========
    {
        "question": "å¦‡ç§‘è¶…å£°æ£€æŸ¥æ˜¯åšä»€ä¹ˆçš„ï¼Ÿæ£€æŸ¥çš„æ—¶å€™ä¼šç–¼å—ï¼Ÿ",
        "entity_type": "examination",
        "difficulty": "easy",
        "expected_weaknesses": ["preparation_requirements", "procedure_details"]
    },
    {
        "question": "åšCTæ£€æŸ¥å‰éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿè¦ç©ºè…¹å—ï¼Ÿ",
        "entity_type": "examination",
        "difficulty": "medium",
        "expected_weaknesses": ["preparation_requirements", "contrast_contraindications"]
    },
    {
        "question": "èƒƒé•œæ£€æŸ¥ç—›è‹¦å—ï¼Ÿå¯ä»¥åšæ— ç—›èƒƒé•œå—ï¼Ÿ",
        "entity_type": "examination",
        "difficulty": "medium",
        "expected_weaknesses": ["anesthesia_options", "procedure_experience"]
    },
    {
        "question": "å¿ƒç”µå›¾èƒ½æŸ¥å‡ºä»€ä¹ˆé—®é¢˜ï¼Ÿ",
        "entity_type": "examination",
        "difficulty": "easy",
        "expected_weaknesses": ["diagnostic_scope", "limitations"]
    },
    {
        "question": "è¡€å¸¸è§„æ£€æŸ¥å‰éœ€è¦ç©ºè…¹å—ï¼Ÿ",
        "entity_type": "examination",
        "difficulty": "easy",
        "expected_weaknesses": ["preparation_requirements", "timing"]
    },
    {
        "question": "æ ¸ç£å…±æŒ¯ï¼ˆMRIï¼‰æ£€æŸ¥æœ‰è¾å°„å—ï¼Ÿ",
        "entity_type": "examination",
        "difficulty": "medium",
        "expected_weaknesses": ["safety_profile", "contraindications"]
    },

    # ========== SURGERIES (4 questions) ==========
    {
        "question": "é˜‘å°¾ç‚æ‰‹æœ¯åå¤šä¹…å¯ä»¥æ­£å¸¸é¥®é£Ÿï¼Ÿ",
        "entity_type": "surgery",
        "difficulty": "medium",
        "expected_weaknesses": ["postoperative_care", "recovery_timeline"]
    },
    {
        "question": "ç—”ç–®æ‰‹æœ¯åä¼šå¤å‘å—ï¼Ÿ",
        "entity_type": "surgery",
        "difficulty": "medium",
        "expected_weaknesses": ["recurrence_rate", "prevention_measures"]
    },
    {
        "question": "ç™½å†…éšœæ‰‹æœ¯é£é™©å¤§å—ï¼Ÿéœ€è¦ä½é™¢å—ï¼Ÿ",
        "entity_type": "surgery",
        "difficulty": "medium",
        "expected_weaknesses": ["surgical_risks", "hospitalization_requirements"]
    },
    {
        "question": "å‰–è…¹äº§åå¤šä¹…å¯ä»¥å†æ€€å­•ï¼Ÿ",
        "entity_type": "surgery",
        "difficulty": "hard",
        "expected_weaknesses": ["postoperative_timeline", "safety_considerations"]
    },

    # ========== VACCINES (4 questions) ==========
    {
        "question": "ç ´ä¼¤é£é’ˆå’Œç ´ä¼¤é£ç–«è‹—æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "entity_type": "vaccine",
        "difficulty": "medium",
        "expected_weaknesses": ["specific_dosage", "administration", "terminology"]
    },
    {
        "question": "HPVç–«è‹—æ‰“å‡ é’ˆï¼Ÿé—´éš”å¤šä¹…ï¼Ÿ",
        "entity_type": "vaccine",
        "difficulty": "easy",
        "expected_weaknesses": ["vaccination_schedule", "dosage_intervals"]
    },
    {
        "question": "æµæ„Ÿç–«è‹—æ¯å¹´éƒ½è¦æ‰“å—ï¼Ÿ",
        "entity_type": "vaccine",
        "difficulty": "easy",
        "expected_weaknesses": ["frequency", "necessity_rationale"]
    },
    {
        "question": "ä¹™è‚ç–«è‹—æ‰“äº†æ²¡æœ‰æŠ—ä½“æ€ä¹ˆåŠï¼Ÿ",
        "entity_type": "vaccine",
        "difficulty": "hard",
        "expected_weaknesses": ["non_responder_management", "booster_strategy"]
    },
]


def call_baseline(question: str, api_client) -> dict:
    """Baseline: Direct DeepSeek API call"""
    messages = [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—å¥åº·åŠ©æ‰‹ã€‚è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å›ç­”æ‚£è€…çš„å¥åº·é—®é¢˜ã€‚"
        },
        {
            "role": "user",
            "content": question
        }
    ]

    start_time = time.time()
    response = api_client.call_deepseek(messages)
    latency = time.time() - start_time

    return {
        "answer": response,
        "latency": latency,
        "method": "baseline"
    }


def call_router(question: str, entity_type: str, api_client, decision_engine, pattern_storage) -> dict:
    """Router: Smart routing with weakness detection + pattern retrieval patterns"""
    # Get routing decision
    decision = decision_engine.get_routing_decision(
        question=question,
        entity_type=entity_type,
        min_confidence=0.7,
        auto_reload=False
    )

    # Build enhanced prompt
    base_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—å¥åº·åŠ©æ‰‹ã€‚è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å›ç­”æ‚£è€…çš„å¥åº·é—®é¢˜ã€‚"
    augmentation = []

    # Add weakness pattern reminders
    if decision.get('has_weaknesses'):
        weakness_reminders = []
        for wp in decision.get('weakness_patterns', [])[:2]:
            weakness_reminders.append(f"- {wp.get('description', '')}")

        if weakness_reminders:
            weakness_section = "\n\nâš ï¸ ç‰¹åˆ«æ³¨æ„ï¼ˆå¸¸è§é—æ¼ç‚¹ï¼‰ï¼š\n" + "\n".join(weakness_reminders)
            augmentation.append(("weakness", weakness_section))

    # Add pattern retrieval patterns if needed
    if decision['use_patterns']:
        # Map singular to plural
        category_map = {
            'disease': 'diseases',
            'examination': 'examinations',
            'surgery': 'surgeries',
            'vaccine': 'vaccines'
        }
        category = category_map.get(entity_type, entity_type)

        relevant_patterns = pattern_storage.retrieve_relevant(
            question=question,
            category=category,
            k=3,
            threshold=0.5
        )

        if relevant_patterns:
            pattern_reminders = []
            for pattern in relevant_patterns:
                pattern_reminders.append(f"- {pattern['description']}")

            pattern_section = "\n\nğŸ“‹ ç›¸å…³çŸ¥è¯†ç‚¹è¡¥å……ï¼š\n" + "\n".join(pattern_reminders)
            augmentation.append(("rag_patterns", pattern_section))

    # Construct final prompt
    augmented_prompt = base_prompt
    for aug_type, aug_content in augmentation:
        augmented_prompt += aug_content

    messages = [
        {
            "role": "system",
            "content": augmented_prompt
        },
        {
            "role": "user",
            "content": question
        }
    ]

    start_time = time.time()
    response = api_client.call_deepseek(messages)
    latency = time.time() - start_time

    return {
        "answer": response,
        "latency": latency,
        "method": "router",
        "routing_decision": decision,
        "augmentation": augmentation
    }


def main():
    """Run extended A/B comparison"""
    logger.info("=" * 80)
    logger.info("Extended A/B Test: Baseline vs Router (20+ questions)")
    logger.info("=" * 80)

    # Initialize components
    logger.info("\n[Setup] Initializing components...")
    api_client = get_api_client()
    decision_engine = get_decision_engine()
    pattern_storage = PatternStorage()

    pattern_count = len(pattern_storage.patterns)
    logger.info(f"âœ“ Loaded {pattern_count} patterns from vector DB")

    # Run comparison
    results = []

    for i, q_data in enumerate(EXTENDED_TEST_QUESTIONS, 1):
        logger.info(f"\n{'=' * 80}")
        logger.info(f"Question {i}/{len(EXTENDED_TEST_QUESTIONS)} [{q_data['difficulty']}]")
        logger.info(f"{'=' * 80}")
        logger.info(f"Q: {q_data['question']}")
        logger.info(f"Entity Type: {q_data['entity_type']}")
        logger.info(f"Expected Weaknesses: {', '.join(q_data.get('expected_weaknesses', []))}")

        # Call baseline
        logger.info("\n--- Baseline ---")
        baseline_result = call_baseline(q_data["question"], api_client)
        logger.info(f"âœ“ Complete ({baseline_result['latency']:.2f}s, {len(baseline_result['answer'])} chars)")

        # Call router
        logger.info("\n--- Router ---")
        router_result = call_router(
            q_data["question"],
            q_data["entity_type"],
            api_client,
            decision_engine,
            pattern_storage
        )
        logger.info(f"âœ“ Complete ({router_result['latency']:.2f}s, {len(router_result['answer'])} chars)")
        logger.info(f"Augmentation: {[a[0] for a in router_result.get('augmentation', [])]}")

        # Compare
        comparison = {
            "question": q_data["question"],
            "entity_type": q_data["entity_type"],
            "difficulty": q_data["difficulty"],
            "expected_weaknesses": q_data.get("expected_weaknesses", []),
            "baseline": {
                "answer": baseline_result["answer"],
                "latency": baseline_result["latency"],
                "answer_length": len(baseline_result["answer"])
            },
            "router": {
                "answer": router_result["answer"],
                "latency": router_result["latency"],
                "answer_length": len(router_result["answer"]),
                "routing_tier": router_result["routing_decision"].get("routing_tier"),
                "weakness_count": len(router_result["routing_decision"].get("weakness_patterns", [])),
                "augmentation_types": [a[0] for a in router_result.get("augmentation", [])]
            },
            "comparison": {
                "length_diff": len(router_result["answer"]) - len(baseline_result["answer"]),
                "latency_diff": router_result["latency"] - baseline_result["latency"]
            }
        }
        results.append(comparison)

    # Save results
    output_dir = Path("outputs/comparisons")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"ab_test_extended_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "metadata": {
                "timestamp": timestamp,
                "total_questions": len(EXTENDED_TEST_QUESTIONS),
                "pattern_count": pattern_count,
                "categories": {
                    "diseases": sum(1 for q in EXTENDED_TEST_QUESTIONS if q["entity_type"] == "disease"),
                    "examinations": sum(1 for q in EXTENDED_TEST_QUESTIONS if q["entity_type"] == "examination"),
                    "surgeries": sum(1 for q in EXTENDED_TEST_QUESTIONS if q["entity_type"] == "surgery"),
                    "vaccines": sum(1 for q in EXTENDED_TEST_QUESTIONS if q["entity_type"] == "vaccine"),
                }
            },
            "results": results
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"\n{'=' * 80}")
    logger.info(f"âœ“ Extended A/B test complete! Results saved to:")
    logger.info(f"  {output_file}")
    logger.info(f"{'=' * 80}")

    # Print summary
    print("\nğŸ“Š SUMMARY\n")
    print(f"Total Questions: {len(EXTENDED_TEST_QUESTIONS)}")
    print(f"  - Diseases: {sum(1 for q in EXTENDED_TEST_QUESTIONS if q['entity_type'] == 'disease')}")
    print(f"  - Examinations: {sum(1 for q in EXTENDED_TEST_QUESTIONS if q['entity_type'] == 'examination')}")
    print(f"  - Surgeries: {sum(1 for q in EXTENDED_TEST_QUESTIONS if q['entity_type'] == 'surgery')}")
    print(f"  - Vaccines: {sum(1 for q in EXTENDED_TEST_QUESTIONS if q['entity_type'] == 'vaccine')}")

    print(f"\nPatterns Used: {pattern_count} patterns in vector DB")

    avg_baseline_len = sum(r['baseline']['answer_length'] for r in results) / len(results)
    avg_router_len = sum(r['router']['answer_length'] for r in results) / len(results)

    print(f"\nAverage Answer Length:")
    print(f"  - Baseline: {avg_baseline_len:.0f} chars")
    print(f"  - Router:   {avg_router_len:.0f} chars ({avg_router_len - avg_baseline_len:+.0f} chars)")

    weakness_used = sum(1 for r in results if r['router']['weakness_count'] > 0)
    rag_used = sum(1 for r in results if 'rag_patterns' in r['router']['augmentation_types'])

    print(f"\nRouter Behavior:")
    print(f"  - Used weakness patterns: {weakness_used}/{len(results)} questions")
    print(f"  - Used pattern retrieval patterns: {rag_used}/{len(results)} questions ({rag_used/len(results)*100:.1f}%)")


if __name__ == "__main__":
    main()
